version: "3.8"

services:
  # PHP service
  php:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: php_app
    volumes:
      - ./php.ini:/usr/local/etc/php/php.ini  # Custom PHP configuration
      - .:/var/www/html  # Mount the project directory
      - ./sessions:/var/lib/php/sessions  # PHP session storage
    working_dir: /var/www/html
    depends_on:
      mysql:
        condition: service_healthy  # Wait for MySQL to be ready before starting
    environment:
      - MYSQL_DATABASE=${MYSQL_DATABASE}
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_PASSWORD=${MYSQL_PASSWORD}

  # Nginx web server
  nginx:
    image: nginx:latest
    container_name: nginx_server
    ports:
      - "${NGINX_PORT}:80"  # Expose Nginx on a configurable port
    volumes:
      - .:/var/www/html  # Serve project files
      - ./nginx.conf:/etc/nginx/conf.d/default.conf  # Custom Nginx configuration
    depends_on:
      - php  # Ensure PHP service is running first

  # MySQL database
  mysql:
    image: mysql:8.0
    container_name: mysql_container
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
      MYSQL_DATABASE: ${MYSQL_DATABASE}
      MYSQL_USER: ${MYSQL_USER}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD}
    ports:
      - "3306:3306"  # Expose MySQL default port
    volumes:
      - mysql_data:/var/lib/mysql  # Persist database data
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 30s
      timeout: 10s
      retries: 3  # Ensure MySQL is ready before dependent services start

  # phpMyAdmin for database management
  phpmyadmin:
    image: phpmyadmin/phpmyadmin
    container_name: phpmyadmin_container
    environment:
      PMA_HOST: mysql  # Connect to MySQL container
      PMA_PORT: 3306
    ports:
      - "8081:80"  # phpMyAdmin UI accessible at http://localhost:8081
    depends_on:
      mysql:
        condition: service_healthy  # Wait for MySQL before starting

  # Redis caching service
  redis:
    image: redis:latest
    container_name: redis_cache
    restart: unless-stopped
    ports:
      - "${REDIS_PORT}:6379"  # Expose Redis
    command: redis-server --appendonly yes  # Enable persistence
    volumes:
      - redis_data:/data  # Persist Redis data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 3

  # MeiliSearch - Full-text search engine
  meilisearch:
    image: getmeili/meilisearch:v1.7
    container_name: meilisearch
    restart: unless-stopped
    environment:
      MEILI_MASTER_KEY: ${MEILI_MASTER_KEY}  # Secure API access
    ports:
      - "7700:7700"
    volumes:
      - meili_data:/meili_data  # Persist search data

  # Prometheus - Monitoring system
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml  # Custom Prometheus config
    ports:
      - "${PROMETHEUS_PORT}:9090"  # Expose Prometheus
    restart: unless-stopped

  # Grafana - Data visualization for metrics
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"  # Access Grafana at http://localhost:3000
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}  # Secure access
    volumes:
      - grafana_data:/var/lib/grafana  # Persist dashboards and settings
    restart: unless-stopped
    depends_on:
      - prometheus  # Ensure Prometheus is running before Grafana starts

  # MinIO - Object storage service (like AWS S3)
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"  # MinIO API
      - "9091:9090"  # MinIO console UI
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}  # Secure access
    volumes:
      - minio_data:/data  # Persist stored objects
    command: server --console-address ":9090" /data
    restart: always

  # RabbitMQ - Message broker for async tasks
  rabbitmq:
    image: rabbitmq:3-management
    container_name: rabbitmq
    ports:
      - "${RABBITMQ_PORT}:5672"  # Message queue
      - "${RABBITMQ_MANAGEMENT_PORT}:15672"  # Management UI
    healthcheck:
      test: ["CMD", "rabbitmqctl", "status"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Apache Kafka - Event streaming platform
  kafka:
    image: confluentinc/cp-kafka
    environment:
      KAFKA_BROKER_ID: ${KAFKA_BROKER_ID}
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:${ZOOKEEPER_PORT}"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:${KAFKA_PORT}
    ports:
      - "${KAFKA_PORT}:9092"  # Expose Kafka
    depends_on:
      - zookeeper  # Ensure Zookeeper is running first
    healthcheck:
      test: ["CMD", "kafka-topics.sh", "--list", "--bootstrap-server", "kafka:${KAFKA_PORT}"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Zookeeper - Required for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: ${ZOOKEEPER_PORT}

  # Portainer - Docker management UI
  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer
    ports:
      - "9000:9000"  # Access Portainer at http://localhost:9000
    restart: always
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock  # Grant access to Docker
      - portainer_data:/data  # Persist Portainer data

  # Netdata - System monitoring tool
  netdata:
    image: netdata/netdata:latest
    container_name: netdata
    pid: host
    network_mode: host  # Use host networking for real-time stats
    cap_add:
      - SYS_PTRACE  # Allow system monitoring
    security_opt:
      - apparmor=unconfined
    restart: always
    volumes:
      - netdata_data:/var/lib/netdata
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /etc/os-release:/host/etc/os-release:ro

  # Nginx Proxy Manager - Reverse proxy with SSL support
  nginx_proxy_manager:
    image: jc21/nginx-proxy-manager:latest
    container_name: nginx_proxy_manager
    restart: always
    ports:
      - "80:80"      # HTTP traffic
      - "443:443"    # HTTPS traffic
      - "81:81"      # Admin panel (http://localhost:81)
    volumes:
      - nginx_proxy_manager_data:/data
      - nginx_proxy_manager_letsencrypt:/etc/letsencrypt

  # Duplicacy - Backup tool for cloud/local storage
  duplicacy:
    image: gilbertchen/duplicacy-web
    container_name: duplicacy
    restart: always
    ports:
      - "3875:3875"  # Web UI (http://localhost:3875)
    volumes:
      - duplicacy_config:/config  # Configuration files
      - duplicacy_cache:/cache  # Cache storage
      - duplicacy_storage:/storage  # Backup storage

  # üöÄ API Gateway (Traefik)
  traefik:
    image: traefik:v2.9
    restart: always
    command:
      - "--api.insecure=true"
      - "--providers.docker=true"
      - "--entrypoints.web.address=:80"
    ports:
      - "80:80"
      - "8080:8080"
    networks:
      - backend
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock

  # üñ•Ô∏è Authentication & Identity (Keycloak)
  keycloak:
    image: quay.io/keycloak/keycloak:latest
    command: ["start-dev"]
    environment:
      KC_DB: postgres
      KC_DB_URL_HOST: postgres
      KC_DB_USERNAME: keycloak
      KC_DB_PASSWORD: keycloakpassword
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: adminpassword
    depends_on:
      - postgres
    ports:
      - "8081:8080"
    networks:
      - backend

  # üíæ PostgreSQL Database
  postgres:
    image: postgres:15
    restart: always
    environment:
      POSTGRES_USER: root
      POSTGRES_PASSWORD: rootpassword
      POSTGRES_DB: app_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - backend

  # üîß Core Backend (Rails)
  rails:
    build: ./rails-backend
    depends_on:
      - postgres
      - redis
    environment:
      DATABASE_URL: postgres://root:rootpassword@postgres:5432/app_db
      REDIS_URL: redis://redis:6379/1
    networks:
      - backend
    volumes:
      - ./rails-backend:/app
    ports:
      - "3000:3000"

  # üì° Real-Time Communication (Node.js + WebSockets)
  websocket:
    build: ./node-realtime
    depends_on:
      - redis
    networks:
      - backend
    ports:
      - "3001:3001"
    volumes:
      - ./node-realtime:/app

  # üî• AI & ML Processing (Python + FastAPI)
  ai_service:
    build: ./python-ai
    depends_on:
      - clickhouse
    networks:
      - backend
    ports:
      - "5000:5000"
    volumes:
      - ./python-ai:/app

  # üìä Analytics Database (ClickHouse)
  clickhouse:
    image: clickhouse/clickhouse-server
    restart: always
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - clickhouse_data:/var/lib/clickhouse
    networks:
      - backend

  # üöÄ Payment Processing (Go + gRPC)
  payments:
    build: ./go-payments
    networks:
      - backend
    ports:
      - "6000:6000"
    volumes:
      - ./go-payments:/app

  # Ethereum Full Node (Geth)
  ethereum-node:
    image: ethereum/client-go  # Uses Geth (Go Ethereum) client
    container_name: geth
    ports:
      - "8545:8545"   # JSON-RPC API port (used for Web3 interactions)
      - "30303:30303" # P2P network communication port
    volumes:
      - ./geth_data:/root/.ethereum  # Persistent blockchain data
    restart: always
    entrypoint: ["geth", "--http", "--http.api", "eth,net,web3"] # Enables RPC API for transactions

  # IPFS (InterPlanetary File System) for decentralized storage
  ipfs:
    image: ipfs/kubo  # Kubo (previously go-ipfs) is the latest IPFS implementation
    container_name: ipfs
    ports:
      - "4001:4001"  # P2P communication
      - "5001:5001"  # API access (used to interact with IPFS)
      - "8080:8080"  # Gateway for accessing stored files
    volumes:
      - ./ipfs_data:/data/ipfs  # Stores files and metadata
    restart: always

  # Bitcoin Lightning Node for fast & low-cost BTC transactions
  lightning:
    image: lightninglabs/lnd  # LND (Lightning Network Daemon) implementation
    container_name: lnd
    ports:
      - "9735:9735"  # P2P Lightning network port
      - "10009:10009" # gRPC API port (used to send transactions)
    volumes:
      - ./lnd_data:/root/.lnd  # Stores wallet and transaction data
    restart: always

  # Hyperledger Fabric Peer Node for private blockchain operations
  hyperledger-peer:
    image: hyperledger/fabric-peer  # Hyperledger Fabric peer node
    container_name: fabric-peer
    ports:
      - "7051:7051"  # Peer communication port
    volumes:
      - ./fabric_peer_data:/var/hyperledger/production  # Stores ledger data
    environment:
      - CORE_PEER_ID=fabric-peer
      - CORE_PEER_ADDRESS=0.0.0.0:7051
    restart: always

  # Chainlink Node for retrieving off-chain data (oracles)
  chainlink:
    image: smartcontract/chainlink
    container_name: chainlink
    ports:
      - "6688:6688"  # API server
    volumes:
      - ./chainlink_data:/chainlink  # Stores blockchain job data
    restart: always
    environment:
      - ETH_CHAIN_ID=1  # Mainnet
      - LINK_CONTRACT_ADDRESS=0x514910771AF9Ca656af840dff83E8264EcF986CA  # Chainlink token contract

  # Web3 Gateway - API Proxy to interact with blockchain services
  web3-gateway:
    image: traefik:v2.5  # Reverse proxy for API calls
    container_name: traefik
    ports:
      - "80:80"      # Standard HTTP port
      - "443:443"    # HTTPS port (if TLS is configured)
    volumes:
      - ./traefik_data:/etc/traefik  # Configuration for routing
    restart: always

networks:
  backend:
    driver: bridge
  frontend:
    driver: bridge
  databases:
    driver: bridge
    
volumes:
  mysql_data:  # Stores MySQL database data to ensure persistence
  redis_data:  # Stores Redis cache data (if used for caching)
  meili_data:  # Stores MeiliSearch index and search data
  grafana_data:  # Stores Grafana dashboards and analytics configurations
  minio_data:  # Stores MinIO object storage data (similar to AWS S3)
  portainer_data:  # Stores Portainer UI settings and Docker management data
  netdata_data:  # Stores Netdata system monitoring logs and metrics
  nginx_proxy_manager_data:  # Stores Nginx Proxy Manager settings and configurations
  nginx_proxy_manager_letsencrypt:  # Stores SSL certificates managed by Nginx Proxy Manager
  duplicacy_config:  # Stores configuration files for Duplicacy backup tool
  duplicacy_cache:  # Stores temporary cache data for Duplicacy (deduplication, chunking, etc.)
  duplicacy_storage:  # Stores actual backup files from Duplicacy (local or cloud backups)
